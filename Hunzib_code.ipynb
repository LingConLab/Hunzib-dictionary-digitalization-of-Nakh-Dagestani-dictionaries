{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pq-fLY2m2nP8",
        "outputId": "65500b16-a94b-48bd-cea0-1bf2dce4957c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m245.8/253.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n",
            "Collecting ru-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_lg-3.8.0/ru_core_news_lg-3.8.0-py3-none-any.whl (513.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m513.4/513.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy3>=1.0.0 (from ru-core-news-lg==3.8.0)\n",
            "  Downloading pymorphy3-2.0.6-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3>=1.0.0->ru-core-news-lg==3.8.0)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-lg==3.8.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-lg==3.8.0) (75.2.0)\n",
            "Downloading pymorphy3-2.0.6-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3, ru-core-news-lg\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.6 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx\n",
        "!python -m spacy download ru_core_news_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx-6XfP72rKc",
        "outputId": "6383c917-e148-4630-e904-a5ff0cf25eb4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Обработка абзацев: 100%|██████████| 9336/9336 [00:24<00:00, 375.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Не удалось извлечь лемму из 23 абзацев. Сохранено в drive/MyDrive/Практика - словари/no_lemma_paragraphs.xlsx\n",
            "Готово! Найдено лемм: 9122, строк всего: 11952\n",
            "Файл сохранён: drive/MyDrive/Практика - словари/result_stage_1.xlsx\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "from docx import Document\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def is_upper_like(word):\n",
        "    \"\"\"Слово капсовое, если все буквы заглавные (и допускается символ №).\"\"\"\n",
        "    letters_only = \"\".join(c for c in word if unicodedata.category(c).startswith('L'))\n",
        "    if not letters_only:\n",
        "        return False\n",
        "    return all(c.isupper() for c in letters_only) or '№' in word\n",
        "\n",
        "\n",
        "\n",
        "def is_class_number(word):\n",
        "    \"\"\"Проверка на классные цифры I, II, III, IV, V, IУ, У\"\"\"\n",
        "    class_numbers = {\"I\", \"II\", \"III\", \"IV\", \"V\", \"IУ\", \"У\"}\n",
        "    parts = re.split(r',\\s*', word.strip(','))\n",
        "    return all(part in class_numbers for part in parts)\n",
        "\n",
        "\n",
        "def is_bold_run(run):\n",
        "    \"\"\"Определяет, является ли run жирным любым способом.\"\"\"\n",
        "    try:\n",
        "        if run.bold:\n",
        "            return True\n",
        "        if hasattr(run.font, \"bold\") and run.font.bold:\n",
        "            return True\n",
        "        if hasattr(run.style, \"font\") and getattr(run.style.font, \"bold\", False):\n",
        "            return True\n",
        "        if hasattr(run.style, \"name\") and \"Bold\" in str(run.style.name):\n",
        "            return True\n",
        "    except Exception:\n",
        "        pass\n",
        "    return False\n",
        "\n",
        "\n",
        "def extract_lemma(text, para=None):\n",
        "    \"\"\"Извлекает лемму из текста абзаца по правилам.\"\"\"\n",
        "    if not text or not text.strip():\n",
        "        return None\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())\n",
        "    tokens = [t for t in text.split() if not is_class_number(t)]\n",
        "    text = \" \".join(tokens)\n",
        "\n",
        "    tokens = text.split()\n",
        "    capseq = []\n",
        "    for tok in tokens:\n",
        "        clean_tok = tok.strip(\"()[]{}.,;!?\")\n",
        "        if is_upper_like(clean_tok):\n",
        "            capseq.append(tok)\n",
        "        elif re.match(r'^[()]*[A-ZА-ЯЁIУV]+[()]*$', tok):\n",
        "            capseq.append(tok)\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    if not capseq:\n",
        "        return None\n",
        "\n",
        "    cap_part = \" \".join(capseq).strip()\n",
        "\n",
        "    if cap_part.endswith(\":\"):\n",
        "        if not para:\n",
        "            return cap_part.rstrip(\":\").strip()\n",
        "\n",
        "        bold_text = \"\"\n",
        "        for run in para.runs:\n",
        "            if is_bold_run(run):\n",
        "                bold_text += run.text + \" \"\n",
        "\n",
        "        bold_text = re.sub(r'\\s+', ' ', bold_text.strip())\n",
        "        bold_words = bold_text.split()\n",
        "\n",
        "        after_colon = text.split(\":\", 1)[1].strip() if \":\" in text else \"\"\n",
        "        after_tokens = after_colon.split()\n",
        "\n",
        "        cap_after = []\n",
        "        for tok in after_tokens:\n",
        "            if tok in bold_words:\n",
        "                cap_after.append(tok)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        if cap_after:\n",
        "            return f\"{cap_part.rstrip(':')}: {' '.join(cap_after)}\"\n",
        "        else:\n",
        "            return cap_part.rstrip(\":\").strip()\n",
        "    else:\n",
        "        return cap_part\n",
        "\n",
        "\n",
        "def extract_definition(text, lemma):\n",
        "    \"\"\"Удаляет лемму из начала текста и возвращает оставшееся определение.\"\"\"\n",
        "    if not text or not lemma:\n",
        "        return None\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', text.strip())\n",
        "    lemma = re.sub(r'\\s+', ' ', lemma.strip())\n",
        "    lemma_clean = lemma.rstrip(\":\").strip()\n",
        "\n",
        "    pattern = re.escape(lemma_clean)\n",
        "    definition = re.sub(rf'^{pattern}[:\\s-]*', '', text, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    return definition if definition else None\n",
        "\n",
        "\n",
        "def split_definitions(definition_text):\n",
        "    \"\"\"\n",
        "    Делит текст словарной статьи на отдельные значения по шаблонам 1., 2., 3. или 1), 2), 3).\n",
        "    \"\"\"\n",
        "    if not definition_text or not isinstance(definition_text, str):\n",
        "        return []\n",
        "\n",
        "    text = re.sub(r'\\s+', ' ', definition_text.strip())\n",
        "    match = re.search(r'\\b1[.)]', text)\n",
        "    if match:\n",
        "        text = text[match.start():]\n",
        "    else:\n",
        "        return [text]\n",
        "\n",
        "    parts = re.split(r'(?:(?<=\\s)|^)\\d+[.)]\\s*', text)\n",
        "    return [p.strip() for p in parts if p.strip()]\n",
        "\n",
        "\n",
        "def extract_morphology(entry: str) -> str:\n",
        "    \"\"\"\n",
        "    Извлекает морфологические данные из словарной статьи:\n",
        "    грамматический класс (I, II, III, IV, V, IУ, У),\n",
        "    мн. (множественное число),\n",
        "    морфологические формы в скобках.\n",
        "    Если встречается '1.' или '1)', обрезает всё, что идёт после них.\n",
        "    \"\"\"\n",
        "\n",
        "    entry = re.split(r'\\b1[.)]', entry, maxsplit=1)[0].strip()\n",
        "\n",
        "    roman_seq_match = re.search(\n",
        "        r'\\b(?:IУ|IV|V|III|II|I|У)(?:\\s*,\\s*(?:IУ|IV|V|III|II|I|У))*\\b', entry\n",
        "    )\n",
        "\n",
        "    first_bracket = re.search(r'\\([^)]*\\)', entry)\n",
        "\n",
        "    bracket_match = None\n",
        "    if first_bracket and '-' in first_bracket.group(0):\n",
        "        bracket_match = first_bracket\n",
        "\n",
        "    plural_text = None\n",
        "    if bracket_match:\n",
        "        start = bracket_match.start()\n",
        "        before = entry[max(0, start - 5):start]\n",
        "        if re.search(r'мн\\.\\s*$', before):\n",
        "            plural_text = \"мн. \" + bracket_match.group(0)\n",
        "        else:\n",
        "            plural_text = bracket_match.group(0)\n",
        "    else:\n",
        "        if re.search(r'\\bмн\\.\\b', entry):\n",
        "            plural_text = \"мн.\"\n",
        "\n",
        "    found_parts = []\n",
        "    if bracket_match:\n",
        "        found_parts.append((bracket_match.start(), plural_text))\n",
        "    if roman_seq_match:\n",
        "        found_parts.append((roman_seq_match.start(), roman_seq_match.group(0)))\n",
        "\n",
        "    found_parts.sort(key=lambda x: x[0])\n",
        "\n",
        "    return \" \".join(p for _, p in found_parts if p).strip()\n",
        "\n",
        "\n",
        "file_path = \"Гунзибско-русский словарь_new.docx\"\n",
        "doc = Document(file_path)\n",
        "\n",
        "rows = []\n",
        "no_lemma = []\n",
        "\n",
        "for para in tqdm(doc.paragraphs, desc=\"Обработка абзацев\"):\n",
        "    text = para.text.strip()\n",
        "    if not text:\n",
        "        continue\n",
        "\n",
        "    lemma = extract_lemma(text, para)\n",
        "    if lemma:\n",
        "        definition = extract_definition(text, lemma)\n",
        "        if definition:\n",
        "            divided_defs = split_definitions(definition)\n",
        "            rows.append({\n",
        "                \"lemma\": lemma,\n",
        "                \"definition\": definition,\n",
        "                \"definition_divided_list\": divided_defs\n",
        "            })\n",
        "    else:\n",
        "        no_lemma.append(text)\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "df = df[df[\"definition\"].notna() & (df[\"definition\"].str.strip() != \"\")].reset_index(drop=True)\n",
        "\n",
        "final_rows = []\n",
        "id_word = 0\n",
        "global_id = 0\n",
        "\n",
        "for _, row in df.iterrows():\n",
        "    id_word += 1\n",
        "    id_meaning = 0\n",
        "    for def_part in row[\"definition_divided_list\"]:\n",
        "        id_meaning += 1\n",
        "        global_id += 1\n",
        "        final_rows.append({\n",
        "            \"id_word\": id_word,\n",
        "            \"id_meaning\": id_meaning,\n",
        "            \"id\": global_id,\n",
        "            \"lemma\": row[\"lemma\"],\n",
        "            \"definition\": row[\"definition\"],\n",
        "            \"definition_divided\": def_part\n",
        "        })\n",
        "\n",
        "df_final = pd.DataFrame(final_rows)\n",
        "\n",
        "morphologies = []\n",
        "clean_divided = []\n",
        "\n",
        "for _, r in df_final.iterrows():\n",
        "    def_text = r[\"definition\"]\n",
        "    div_text = r[\"definition_divided\"]\n",
        "\n",
        "    morph_def = extract_morphology(def_text)\n",
        "    morph_div = extract_morphology(div_text)\n",
        "\n",
        "    new_div_text = div_text\n",
        "    morphology = None\n",
        "\n",
        "    if not morph_def and not morph_div:\n",
        "        morphology = None\n",
        "    elif morph_def == morph_div or not morph_div:\n",
        "        morphology = morph_def\n",
        "\n",
        "        if morph_def:\n",
        "            new_div_text = re.sub(re.escape(morph_def), \"\", div_text, flags=re.IGNORECASE).strip()\n",
        "    else:\n",
        "        morphology = f\"{morph_def or ''}; {morph_div or ''}\".strip(\"; \")\n",
        "        if morph_div:\n",
        "            new_div_text = re.sub(re.escape(morph_div), \"\", div_text, flags=re.IGNORECASE).strip()\n",
        "\n",
        "    morphologies.append(morphology)\n",
        "    clean_divided.append(re.sub(r'\\s+', ' ', new_div_text.strip()) if new_div_text else None)\n",
        "\n",
        "df_final[\"morphology\"] = morphologies\n",
        "df_final[\"definition_divided\"] = clean_divided\n",
        "\n",
        "output_path = \"result_stage_1.xlsx\"\n",
        "df_final.to_excel(output_path, index=False)\n",
        "\n",
        "if no_lemma:\n",
        "    df_no = pd.DataFrame({\"no_lemma_paragraph\": no_lemma})\n",
        "    no_path = \"no_lemma_paragraphs.xlsx\"\n",
        "    df_no.to_excel(no_path, index=False)\n",
        "    print(f\"Не удалось извлечь лемму из {len(no_lemma)} абзацев. Сохранено в {no_path}\")\n",
        "else:\n",
        "    print(\"Все абзацы успешно обработаны!\")\n",
        "\n",
        "print(f\"Готово! Найдено лемм: {df_final['id_word'].nunique()}, строк всего: {len(df_final)}\")\n",
        "print(f\"Файл сохранён: {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hifzHcf923TO",
        "outputId": "9977ceea-9318-434e-8228-1d0d61ba1878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Файл успешно сохранён: drive/MyDrive/Практика - словари/result_stage_2.xlsx\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"ru_core_news_lg\", disable=[\"ner\"])\n",
        "\n",
        "IGNORED_LABELS = [\n",
        "    \"воен.\", \"перен.\", \"спец.\", \"разг.\", \"уст.\", \"ирон.\", \"поэт.\", \"анат.\", \"зоол.\",\n",
        "    \"бот.\", \"рел.\", \"астр.\", \"ген.\", \"физиол.\", \"ист.\", \"брaн.\", \"миф.\", \"фольк.\",\n",
        "    \"с.-х.\", \"геогр.\", \"спорт.\", \"кулин.\", \"дет.\", \"игр.\", \"неодобр.\", \"пренебр.\", \"шутл.\"\n",
        "]\n",
        "\n",
        "def clean_phrase(phrase: str) -> str:\n",
        "    \"\"\"\n",
        "    Убирает стилистические/тематические метки в начале строки (воен., перен., уст. и т.п.)\n",
        "    \"\"\"\n",
        "    phrase = phrase.strip()\n",
        "    pattern = r\"^(?:\" + \"|\".join(re.escape(l) for l in IGNORED_LABELS) + r\")\\s*\"\n",
        "    phrase_cleaned = re.sub(pattern, \"\", phrase, flags=re.IGNORECASE)\n",
        "    return phrase_cleaned\n",
        "\n",
        "\n",
        "def detect_pos(text: str):\n",
        "    \"\"\"\n",
        "    Определяет часть речи для строки по описанным правилам.\n",
        "    Возвращает кортеж (pos, detected_by)\n",
        "    \"\"\"\n",
        "\n",
        "    if not isinstance(text, str) or not text.strip():\n",
        "        return \"NA\", \"none\"\n",
        "\n",
        "    has_expression = any(x in text for x in [\"♦\", \"погов.\", \"посл.\"])\n",
        "\n",
        "    markers = {\n",
        "        \"сущ.\": \"noun\",\n",
        "        \"им.\": \"noun\",\n",
        "        \"прил.\": \"adj\",\n",
        "        \"адъектив.\": \"adj\",\n",
        "        \"качеств.\": \"adj\",\n",
        "        \"гл.\": \"verb\",\n",
        "        \"масд.\": \"masdar\",\n",
        "        \"инф.\": \"verb\",\n",
        "        \"прич.\": \"verb\",\n",
        "        \"деепр.\": \"verb\",\n",
        "        \"учащ.\": \"verb\",\n",
        "        \"понуд.\": \"verb\",\n",
        "        \"учащ. понуд.\": \"verb\",\n",
        "        \"союз\": \"conj\",\n",
        "        \"послелог\": \"post\",\n",
        "        \"предлог\": \"adp\",\n",
        "        \"част.\": \"part\",\n",
        "        \"межд.\": \"intj\",\n",
        "        \"звукоподр.\": \"onomatope\",\n",
        "        \"звукосимв.\": \"onomatope\",\n",
        "        \"числ.\": \"num\",\n",
        "        \"мест.\": \"pron\",\n",
        "        \"нареч.\": \"adv\",\n",
        "    }\n",
        "\n",
        "    first_part = text.split(\";\")[0].strip()\n",
        "\n",
        "    for marker, pos in markers.items():\n",
        "        if marker in first_part:\n",
        "            result = pos\n",
        "            detected_by = \"source\"\n",
        "            break\n",
        "    else:\n",
        "        # если маркеров нет, анализируем через spaCy\n",
        "        detected_by = \"spacy_model\"\n",
        "        phrases = [p.strip() for p in first_part.split(\",\") if p.strip()]\n",
        "        pos_counts = {}\n",
        "\n",
        "        for phrase in phrases:\n",
        "            phrase_clean = clean_phrase(phrase)\n",
        "            doc = nlp(phrase_clean)\n",
        "\n",
        "            if len(doc) == 0:\n",
        "                pos_counts[\"NA\"] = pos_counts.get(\"NA\", 0) + 1\n",
        "                continue\n",
        "\n",
        "            root = next((t for t in doc if t.dep_ == \"ROOT\"), doc[0])\n",
        "            main_pos = root.pos_\n",
        "\n",
        "            # если фраза начинается с предлога, а root — существительное,\n",
        "            # считаем конструкцию наречием\n",
        "            starts_with_prep = len(doc) > 0 and doc[0].pos_ == \"ADP\"\n",
        "            if starts_with_prep and main_pos == \"NOUN\":\n",
        "                main_pos = \"ADV\"\n",
        "\n",
        "            pos_map = {\n",
        "                \"NOUN\": \"noun\",\n",
        "                \"PROPN\": \"noun\",\n",
        "                \"ADJ\": \"adj\",\n",
        "                \"VERB\": \"verb\",\n",
        "                \"AUX\": \"verb\",\n",
        "                \"ADV\": \"adv\",\n",
        "                \"ADP\": \"adp\",\n",
        "                \"CCONJ\": \"conj\",\n",
        "                \"SCONJ\": \"conj\",\n",
        "                \"PRON\": \"pron\",\n",
        "                \"NUM\": \"num\",\n",
        "                \"PART\": \"part\",\n",
        "                \"INTJ\": \"intj\",\n",
        "                \"SYM\": \"onomatope\",\n",
        "                \"X\": \"NA\",\n",
        "            }\n",
        "\n",
        "            mapped = pos_map.get(main_pos, \"NA\")\n",
        "            pos_counts[mapped] = pos_counts.get(mapped, 0) + 1\n",
        "\n",
        "        if pos_counts:\n",
        "            result = max(pos_counts, key=pos_counts.get)\n",
        "        else:\n",
        "            result = \"NA\"\n",
        "\n",
        "    if has_expression:\n",
        "        result += \"; expression\"\n",
        "\n",
        "    return result, detected_by\n",
        "\n",
        "\n",
        "def process_excel(input_path: str, output_path: str):\n",
        "    df = pd.read_excel(input_path)\n",
        "\n",
        "    required_cols = [\"definition_divided\"]\n",
        "    for col in required_cols:\n",
        "        if col not in df.columns:\n",
        "            raise ValueError(f\"В файле отсутствует обязательный столбец: {col}\")\n",
        "\n",
        "    results = df[\"definition_divided\"].apply(detect_pos)\n",
        "    df[\"pos\"] = results.apply(lambda x: x[0])\n",
        "    df[\"detected_by\"] = results.apply(lambda x: x[1])\n",
        "\n",
        "    df.to_excel(output_path, index=False)\n",
        "    print(f\"Файл успешно сохранён: {output_path}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    process_excel(\n",
        "        \"result_stage_1.xlsx\",\n",
        "        \"result_stage_2.xlsx\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRlBJC6I2-iW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def replace_number_sign(text: str) -> str:\n",
        "    \"\"\"Заменяет все символы № в строке на надстрочный ᵸ.\"\"\"\n",
        "    return text.replace('№', 'ᵸ')\n",
        "\n",
        "our_data = pd.read_excel(\"result_stage_2.xlsx\")\n",
        "our_data = our_data.map(lambda x: replace_number_sign(x) if isinstance(x, str) else x)\n",
        "our_data['lemma'] = our_data['lemma'].str.lower()\n",
        "our_data['language'] = 'Hunzib'\n",
        "our_data['glottocode'] = 'hunz1247'\n",
        "our_data['annotator'] = 'Alesya Voinskaya'\n",
        "our_data['reference'] = 'Khalilov 2026'\n",
        "our_data.to_excel(\"result_stage_2.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWmGB0MF1MBg",
        "outputId": "edd14e17-a4cb-4eab-9d9c-8d0870b2c5a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "бəдǡ̄кiекi/а\n",
            "гьазарс̄ə\n",
            "гьǡ̄̄сбурти\n",
            "kьaлaki\n",
            "kьȧрхiаh\n",
            "кючi\n",
            "кючi\n",
            "мийавдāрл'ьи\n",
            "нишерекǡ̄\n",
            "т̄ǡ̄цi/а\n",
            "т̄ǡ̄цiекi/а\n",
            "т̄ǡ̄цiекiерлъи\n",
            "тіагьарат\n",
            "тіарикъат\n",
            "хіаким-сукіу\n",
            "цǡ̄-цȧсс/ə\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from typing import Dict\n",
        "import unicodedata\n",
        "\n",
        "vowels = {\n",
        "\n",
        "    \"а̄ᵸ\": \"ãː\",\n",
        "    \"а̄\": \"aː\",\n",
        "    \"аᵸ\": \"ã\",\n",
        "    \"а\": \"a\",\n",
        "\n",
        "    \"ā\": \"aː\",\n",
        "\n",
        "    \"ǡᵸ\": \"ɑ̃\",\n",
        "    \"ǡ\": \"ɑː\",\n",
        "    \"ȧᵸ\": \"ɑ̃\",\n",
        "    \"ȧ\": \"ɑ\",\n",
        "\n",
        "    \"ǡ\": \"ɑː\",\n",
        "\n",
        "    \"ēᵸ\": \"ẽː\",\n",
        "    \"ē\": \"eː\",\n",
        "    \"еᵸ\": \"ẽ\",\n",
        "    \"е\": \"e\",\n",
        "    \"ё\": \"e\",\n",
        "\n",
        "    \"ӣᵸ\": \"ĩː\",\n",
        "    \"ӣ\": \"iː\",\n",
        "    \"иᵸ\": \"ĩ\",\n",
        "    \"и\": \"i\",\n",
        "\n",
        "    \"ӣ\": \"iː\",\n",
        "\n",
        "    \"ōᵸ\": \"õː\",\n",
        "    \"ō\": \"oː\",\n",
        "    \"оᵸ\": \"õ\",\n",
        "    \"о\": \"o\",\n",
        "\n",
        "    \"ӯᵸ\": \"ũː\",\n",
        "    \"ӯ\": \"uː\",\n",
        "    \"уᵸ\": \"ũ\",\n",
        "    \"у\": \"u\",\n",
        "\n",
        "    \"ӯ\": \"uː\",\n",
        "\n",
        "    \"э̄ᵸ\": \"ʔẽː\",\n",
        "    \"э̄\": \"ʔeː\",\n",
        "    \"эᵸ\": \"ʔẽ\",\n",
        "    \"э\": \"ʔe\",\n",
        "\n",
        "    \"ы̄ᵸ\": \"ɨ̃ː\",\n",
        "    \"ы̄\": \"ɨː\",\n",
        "    \"ыᵸ\": \"ɨ̃\",\n",
        "    \"ы\": \"ɨ\",\n",
        "    \"ьi\": \"ɨ\",\n",
        "\n",
        "    \"ә̄ᵸ\": \"ə̃ː\",\n",
        "    \"ә̄\": \"əː\",\n",
        "    \"әᵸ\": \"ə̃\",\n",
        "    \"ǝ\": \"ə\",\n",
        "\n",
        "    \"ә̄ᵸ\": \"ə̃ː\",\n",
        "    \"ә̄\": \"əː\",\n",
        "    \"әᵸ\": \"ə̃\",\n",
        "    \"ә\": \"ə\",\n",
        "\n",
        "    \"ǝ̄ᵸ\": \"ə̃ː\",\n",
        "    \"ǝ̄\": \"əː\",\n",
        "    \"ǝᵸ\": \"ə̃\",\n",
        "    \"ǝ\": \"ə\",\n",
        "\n",
        "    \"ə̄\": \"əː\"\n",
        "}\n",
        "\n",
        "consonants = {\n",
        "\n",
        "    \"б\": \"b\",\n",
        "    \"в\": \"w\",\n",
        "    \"г\": \"g\",\n",
        "    \"гъ\": \"ʁ\",\n",
        "    \"гь\": \"h\",\n",
        "    \"гI\": \"ʕ\",\n",
        "    \"гi\": \"ʕ\",\n",
        "    \"гі\": \"ʕ\",\n",
        "    \"д\": \"d\",\n",
        "    \"ж\": \"ž\",         # ž [ʒ]\n",
        "    \"з\": \"z\",\n",
        "\n",
        "    \"й\": \"j\",\n",
        "    \"к\": \"k\",\n",
        "    \"къ\": \"qʼ\",\n",
        "    \"кь\": \"ƛ’\",       # tɬ’\n",
        "    \"кI\": \"kʼ\",\n",
        "    \"кi\": \"kʼ\",\n",
        "\n",
        "    \"л\": \"l\",\n",
        "    \"лъ\": \"ɬ\",\n",
        "    \"лъ\": \"ɬ\",\n",
        "    \"лI\": \"ƛ\",        # tɬ\n",
        "    \"лi\": \"ƛ\",\n",
        "\n",
        "    \"м\": \"m\",\n",
        "    \"н\": \"n\",\n",
        "\n",
        "    \"п\": \"p\",\n",
        "    \"пI\": \"pʼ\",\n",
        "    \"пi\": \"pʼ\",\n",
        "\n",
        "    \"р\": \"r\",\n",
        "    \"с\": \"s\",\n",
        "\n",
        "    \"т\": \"t\",\n",
        "    \"тI\": \"tʼ\",\n",
        "    \"тi\": \"tʼ\",\n",
        "\n",
        "    \"х\": \"χ\",\n",
        "    \"хъ\": \"q\",        # в таблице перемешаны q и qχ - записала как q\n",
        "    \"хь\": \"x\",\n",
        "    \"хI\": \"ħ\",\n",
        "    \"хi\": \"ħ\",\n",
        "    \"хі\":\"ħ\",\n",
        "\n",
        "    \"ц\": \"c\",         # c [ts]\n",
        "    \"цI\": \"cʼ\",       # c’ [tsʼ]\n",
        "    \"цi\": \"cʼ\",\n",
        "\n",
        "    \"ч\": \"č\",\n",
        "    \"чI\": \"čʼ\",\n",
        "    \"чi\": \"čʼ\",\n",
        "\n",
        "    \"ш\": \"š\",\n",
        "\n",
        "    \"ъ\": \"ʔ\",\n",
        "\n",
        "    # знаки\n",
        "\n",
        "    \"-\": \"-\",  # добавили также другие знаки, т.к. хотим их сохранить\n",
        "    \"(\": \"(\",\n",
        "    \")\": \")\",\n",
        "    \"§§\": \"§§\"\n",
        "}\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "from typing import Dict\n",
        "\n",
        "def transliterate_and_check(s: str,\n",
        "                            vowels_map: Dict[str, str] = vowels,\n",
        "                            cons_map: Dict[str, str] = consonants) -> str:\n",
        "    original_s = s\n",
        "\n",
        "    s = s.replace(\"!\", \"\").replace(\";\", \"\").replace(\" iⅴ\", \"\").replace(\"...\", \"\").replace(\" i.\", \"\")\n",
        "    s = s.replace(\"//\", \"§§\")\n",
        "    s = s.replace(\"/\", \"\")\n",
        "    if \":\" in s:\n",
        "        s = s.split(\":\")[0]\n",
        "    if \" (\" in s:\n",
        "        s = s.split(\" (\")[0]\n",
        "\n",
        "    s = unicodedata.normalize(\"NFD\", s)\n",
        "\n",
        "    s = re.sub(r\"\\d\", \"\", s)\n",
        "\n",
        "    # Удаляем ударения и подобные диакритические знаки\n",
        "    accents_to_remove = {\n",
        "        '\\u0301',\n",
        "        '\\u0300',\n",
        "        '\\u030B',\n",
        "        '\\u0341',\n",
        "        '\\u02CA',\n",
        "        '\\u02CB',\n",
        "    }\n",
        "    s = ''.join(c for c in s if c not in accents_to_remove)\n",
        "\n",
        "    # склеиваем надстрочный ᵸ с предыдущей гласной\n",
        "    vowels_set = set(vowels_map.keys())\n",
        "    result = \"\"\n",
        "    for i, c in enumerate(s):\n",
        "        if c == \"ᵸ\" and i > 0 and s[i-1] in vowels_set:\n",
        "            # заменяем предыдущий символ на \"гласная+ᵸ\"\n",
        "            result = result[:-1] + s[i-1] + \"ᵸ\"\n",
        "        elif c != \"ᵸ\":\n",
        "            result += c\n",
        "        # если ᵸ без гласной перед ним — просто игнорируем\n",
        "    s = result\n",
        "\n",
        "    s = unicodedata.normalize(\"NFC\", s)\n",
        "\n",
        "    mapping = {**vowels_map, **cons_map}\n",
        "\n",
        "    keys_sorted = sorted(mapping.keys(), key=len, reverse=True)\n",
        "    escaped_keys = [re.escape(k) for k in keys_sorted]\n",
        "    pattern = re.compile(\"|\".join(escaped_keys))\n",
        "\n",
        "    def _repl(m: re.Match) -> str:\n",
        "        return mapping[m.group(0)] + \"-\"\n",
        "\n",
        "    replaced = pattern.sub(_repl, s)\n",
        "\n",
        "\n",
        "    test_var = replaced\n",
        "    values_sorted = sorted(set(mapping.values()), key=len, reverse=True)\n",
        "    for val in values_sorted:\n",
        "        if val == \"\":\n",
        "            continue\n",
        "        test_var = test_var.replace(val, \"\")\n",
        "    if test_var.strip() != \"\":\n",
        "          print(original_s)\n",
        "        #else:\n",
        "          #print(\"Необработанный остаток после удаления значений словарей:\")\n",
        "          #print(repr(test_var))\n",
        "          #print(f\"Изначальное слово: {original_s!r}\")\n",
        "          #raise RuntimeError(\"Некоторые символы не были заменены соответствующими значениями словарей.\")\n",
        "\n",
        "\n",
        "    replaced_with_dashes = replaced.strip()\n",
        "    if replaced_with_dashes.endswith(\"-\"):\n",
        "        replaced_with_dashes = replaced_with_dashes[:-1]\n",
        "    replaced_with_dashes = replaced_with_dashes.replace(\"-ʼ\", \"ʼ\")\n",
        "    replaced_with_dashes = replaced_with_dashes.replace(\"(-\", \"(\")\n",
        "    replaced_with_dashes = replaced_with_dashes.replace(\"-)\", \")\")\n",
        "    replaced_with_dashes = replaced_with_dashes.replace(\"---\", \" \")\n",
        "    replaced_with_dashes = replaced_with_dashes.replace(\"-§§-\", \"//\")\n",
        "\n",
        "    return replaced_with_dashes\n",
        "\n",
        "\n",
        "df = pd.read_excel(\"result_stage_2.xlsx\")\n",
        "df[\"ipa\"] = df[\"lemma\"].apply(lambda x: transliterate_and_check(str(x)))\n",
        "df.to_excel(\"result_stage_3.xlsx\", index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6MUvhACsxC7"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel(\"result_stage_3.xlsx\")\n",
        "\n",
        "new_order = ['id_word',\t'id_meaning',\t'id',\t'lemma',\t'morphology',\t'definition',\t'definition_divided',\t'glottocode',\t'reference', 'language', 'pos', 'detected_by']\n",
        "\n",
        "df = df[new_order]\n",
        "\n",
        "df.to_excel(\"Hunzib_dictionary.xlsx\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
